{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "DEBUG = False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import os\n",
    "import librosa\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import timm\n",
    "\n",
    "import random\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint#, StochasticWeightAveraging\n",
    "# from pl_bolts.optimizers.lr_scheduler import LinearWarmupCosineAnnealingLR\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "from typing import Optional\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import concurrent.futures\n",
    "import time\n",
    "import seaborn as sns"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# from torchvision import transforms"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# def mono_to_color(X: np.ndarray, mean=0.5, std=0.5, eps=1e-6):\n",
    "#     trans = transforms.Compose([transforms.ToPILImage(),\n",
    "#                                         transforms.Resize([224, 448]), transforms.ToTensor(),\n",
    "#                                         transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "#     X = np.stack([X, X, X], axis=-1)\n",
    "#     V = (255 * X).astype(np.uint8)\n",
    "#     V = (trans(V)+1)/2\n",
    "#     return V"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import albumentations as A\n",
    "from albumentations.augmentations.crops.transforms import RandomCrop\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "def get_transforms(*, data):\n",
    "    \n",
    "    if data == 'train':\n",
    "        return A.Compose([\n",
    "            A.Resize(224, 448),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "\n",
    "    elif data == 'valid':\n",
    "        return A.Compose([\n",
    "            A.Resize(224, 448),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "\n",
    "    elif data == 'None':\n",
    "        return A.Compose([\n",
    "            A.Resize(224, 448),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "\n",
    "    elif data == 'test':\n",
    "        return A.Compose([\n",
    "            A.Resize(224, 448),\n",
    "            ToTensorV2(),\n",
    "        ])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "BASE_DIR = '../data/'\n",
    "TRAIN_PATH = os.path.join(BASE_DIR, 'w2n_train')\n",
    "TEST_PATH = os.path.join(BASE_DIR, 'w2n_test')\n",
    "def read_wav2mel(path):\n",
    "        # y, sr = librosa.load(path) # y: audio signal, sr = sampling rate\n",
    "        y = np.load(path)\n",
    "\n",
    "        n_fft = 892\n",
    "        win_length = 892\n",
    "        hop_length = 245\n",
    "        n_mels = 224\n",
    "        sr = 22050\n",
    "\n",
    "        D = np.abs(librosa.stft(y, n_fft=n_fft, win_length = win_length, hop_length=hop_length))\n",
    "        mel_spec = librosa.feature.melspectrogram(S=D, sr=sr, n_mels=n_mels, hop_length=hop_length, win_length=win_length)\n",
    "\n",
    "        return mel_spec\n",
    "\n",
    "def mfcc_extract(path):\n",
    "    try:\n",
    "        # y, sr  = librosa.load(filename, sr = 44100)\n",
    "        # path = os.path.join(TRAIN_PATH, filename) + '.npy'\n",
    "        y = np.load(path)\n",
    "        sr = 22050\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13, n_fft=892, hop_length=int(245))\n",
    "        return mfcc\n",
    "    except:\n",
    "        return\n",
    "\n",
    "\n",
    "def get_mfcc(data, trans=None, is_train=True):\n",
    "    if is_train:\n",
    "        path, label= data\n",
    "        path = os.path.join(TRAIN_PATH, path) + '.npy'\n",
    "        mfcc_feat = mfcc_extract(path)\n",
    "        \n",
    "        if not trans is None:\n",
    "            data = trans(image=mfcc_feat)\n",
    "            mfcc_feat = data['image']\n",
    "        \n",
    "        return mfcc_feat, label-1\n",
    "    else:\n",
    "        path= data\n",
    "        path = os.path.join(TEST_PATH, path) + '.npy'\n",
    "        mfcc_feat = mfcc_extract(path)\n",
    "        \n",
    "        if not trans is None:\n",
    "            data = trans(image=mfcc_feat)\n",
    "            mfcc_feat = data['image']\n",
    "        \n",
    "        return mfcc_feat\n",
    "\n",
    "\n",
    "def get_mel(data, trans=None, is_train=True):\n",
    "    if is_train:\n",
    "        path, label= data\n",
    "        path = os.path.join(TRAIN_PATH, path) + '.npy'\n",
    "        mel = read_wav2mel(path)\n",
    "        \n",
    "        if not trans is None:\n",
    "            data = trans(image=mel)\n",
    "            mel = data['image']\n",
    "        \n",
    "        return mel, label-1\n",
    "    else:\n",
    "        path= data\n",
    "        path = os.path.join(TEST_PATH, path) + '.npy'\n",
    "        mel = read_wav2mel(path)\n",
    "        \n",
    "        if not trans is None:\n",
    "            data = trans(image=mel)\n",
    "            mel = data['image']\n",
    "        \n",
    "        return mel\n",
    "\n",
    "class WavDatset(Dataset):\n",
    "\n",
    "    def __init__(self, df, trans=None, is_train=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.df = df\n",
    "        self.is_train = is_train\n",
    "        self.trans = trans\n",
    "\n",
    "        if self.is_train:\n",
    "            self.data = []\n",
    "            print(\"Load Train mode dataset ...\")\n",
    "            start = time.time()\n",
    "            with ThreadPoolExecutor(max_workers=60) as executor:\n",
    "                future_to_list = {executor.submit(get_mel, j, self.trans, self.is_train): j for j in self.df[['file_name', 'age_']].values}\n",
    "                for future in concurrent.futures.as_completed(future_to_list):\n",
    "                    j = future_to_list[future]\n",
    "                    try:\n",
    "                        self.data.append(future.result())\n",
    "                        \n",
    "                    except Exception as exc:\n",
    "                        print('%r generated an exception: %s' % (j, exc))\n",
    "            elapsed = time.time() - start\n",
    "            print(\"ThreadPool Elapsed {}\".format(elapsed))\n",
    "\n",
    "        else:\n",
    "            self.data = []\n",
    "            print(\"Load Inference mode dataset ...\")\n",
    "            start = time.time()\n",
    "            with ThreadPoolExecutor(max_workers=60) as executor:\n",
    "                future_to_list = {executor.submit(get_mel, str(j), self.trans, self.is_train): j for j in self.df['file_name'].values}\n",
    "                for future in concurrent.futures.as_completed(future_to_list):\n",
    "                    j = future_to_list[future]\n",
    "                    try:\n",
    "                        self.data.append(future.result())\n",
    "                        \n",
    "                    except Exception as exc:\n",
    "                        print('%r generated an exception: %s' % (j, exc))\n",
    "            elapsed = time.time() - start\n",
    "            print(\"ThreadPool Elapsed {}\".format(elapsed))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        if self.is_train:\n",
    "            return self.data[index][0], self.data[index][1]\n",
    "        else:\n",
    "            return self.data[index]\n",
    "    \n",
    "print(\"Dataset Test as Train mode ...\")\n",
    "df = pd.read_csv('../data/train_label.csv')[:100]\n",
    "trans = get_transforms(data='train')\n",
    "dataset = WavDatset(df, trans=trans)\n",
    "mel, label = dataset[1]\n",
    "print(mel.shape)\n",
    "print(label)\n",
    "\n",
    "print(\"Dataset Test as Inference mode ...\")\n",
    "infer_df = pd.read_csv('../data/sample_submission.csv')[:100]\n",
    "infer_trans = get_transforms(data='test')\n",
    "infer_dataset = WavDatset(infer_df, trans=infer_trans, is_train=False)\n",
    "infer_mel = infer_dataset[1]\n",
    "print(infer_mel.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset Test as Train mode ...\n",
      "Load Train mode dataset ...\n",
      "ThreadPool Elapsed 2.02404522895813\n",
      "torch.Size([1, 224, 448])\n",
      "4\n",
      "Dataset Test as Inference mode ...\n",
      "Load Inference mode dataset ...\n",
      "ThreadPool Elapsed 1.930243730545044\n",
      "torch.Size([1, 224, 448])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# model =  timm.create_model(model_name='mobilenetv3_large_100', pretrained=True, in_chans=1)\n",
    "# model.classifier = nn.Linear(model.classifier.in_features, 7)\n",
    "# sample = torch.rand(32, 1, 128, 128)\n",
    "# out = model(sample)\n",
    "# out.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# class ImgModel(nn.Module):\n",
    "#     def __init__(self, backbone='mobilenetv3_large_100'):\n",
    "#         super(ImgModel, self).__init__()\n",
    "\n",
    "#         self.backbone = timm.create_model(model_name=backbone, pretrained=True, in_chans=1)\n",
    "#         # self.backbone.classifier = nn.Linear(self.backbone.classifier.in_features, 7)\n",
    "\n",
    "\n",
    "#         self.pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "#         out_features = self.backbone.classifier.in_features\n",
    "#         self.fc = nn.Linear(out_features, 7)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out = self.backbone.forward_features(x)\n",
    "#         out = self.pool(out)\n",
    "#         out = self.fc(out[:,:,0,0])\n",
    "#         return out\n",
    "\n",
    "# # unit test\n",
    "# print(\"Model Test ...\")\n",
    "# model = ImgModel()\n",
    "# sample = torch.rand(32, 1, 128, 128)\n",
    "# out = model(sample)\n",
    "# print(out.shape)\n",
    "# # print(model)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "class ImgModel(nn.Module):\n",
    "    def __init__(self, backbone='tf_efficientnet_b0_ns'):\n",
    "        super(ImgModel, self).__init__()\n",
    "\n",
    "        self.backbone = timm.create_model(model_name=backbone, pretrained=True, in_chans=1)\n",
    "        self.pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        out_features = self.backbone.classifier.in_features\n",
    "        self.fc = nn.Linear(out_features, 7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.backbone.forward_features(x)\n",
    "        out = self.pool(out)\n",
    "        out = self.fc(out[:,:,0,0])\n",
    "\n",
    "        return out\n",
    "\n",
    "# unit test\n",
    "print(\"Model Test ...\")\n",
    "model = ImgModel()\n",
    "sample = torch.rand(32, 1, 128, 128)\n",
    "out = model(sample)\n",
    "print(out.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model Test ...\n",
      "torch.Size([32, 7])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "class GradualWarmupSchedulerV2(GradualWarmupScheduler):\n",
    "    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n",
    "        super(GradualWarmupSchedulerV2, self).__init__(optimizer, multiplier, total_epoch, after_scheduler)\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch > self.total_epoch:\n",
    "            if self.after_scheduler:\n",
    "                if not self.finished:\n",
    "                    self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "                    self.finished = True\n",
    "                return self.after_scheduler.get_lr()\n",
    "            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "        if self.multiplier == 1.0:\n",
    "            return [base_lr * (float(self.last_epoch) / self.total_epoch) for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "class LitClassifier(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    >>> LitClassifier(Backbone())  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\n",
    "    LitClassifier(\n",
    "      (backbone): ...\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        scale_list = [0.25, 0.5], # 0.125, \n",
    "        backbone: Optional[ImgModel] = None,\n",
    "        learning_rate: float = 0.0001,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=['backbone'])\n",
    "        if backbone is None:\n",
    "            backbone = ImgModel()\n",
    "        self.backbone = backbone\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, batch):\n",
    "        out = self.backbone.forward_features(batch)\n",
    "        out = self.pool(out)\n",
    "        out = self.fc(out[:,:,0,0])\n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        x, y = batch\n",
    "        labels = y.long()\n",
    "\n",
    "        output = self.backbone(x)\n",
    "\n",
    "        loss = self.criterion(output, labels)\n",
    "        \n",
    "        try:\n",
    "            pred = output.detach().cpu()\n",
    "            pred = torch.argmax(pred, dim=1)\n",
    "            acc=accuracy_score(labels.detach().cpu(), pred) \n",
    "\n",
    "            self.log(\"acc\", acc, on_step= True, prog_bar=True, logger=True)\n",
    "            self.log(\"Train Loss\", loss, on_step= True,prog_bar=True, logger=True)\n",
    "        \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        return {\"loss\": loss, \"predictions\": output.detach().cpu(), \"labels\": labels.detach().cpu()}\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "\n",
    "        preds = []\n",
    "        labels = []\n",
    "        \n",
    "        for output in outputs:\n",
    "            \n",
    "            preds += output['predictions']\n",
    "            labels += output['labels']\n",
    "\n",
    "        labels = torch.stack(labels)\n",
    "        preds = torch.stack(preds)\n",
    "        \n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        output = self.backbone(x)\n",
    "        labels = y.long()\n",
    "\n",
    "        loss = self.criterion(output, labels)\n",
    "        \n",
    "        self.log('val_loss', loss, on_step= True, prog_bar=True, logger=True)\n",
    "        return {\"predictions\": output, \"labels\": labels}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "\n",
    "        preds = []\n",
    "        labels = []\n",
    "        for output in outputs:\n",
    "            # preds.append(output['predictions'])\n",
    "            # labels.append(output['labels'])\n",
    "            preds += output['predictions']\n",
    "            labels += output['labels']\n",
    "        labels = torch.stack(labels)\n",
    "        preds = torch.stack(preds)\n",
    "        preds = preds.detach().cpu()\n",
    "        preds = torch.argmax(preds, dim=1)\n",
    "        val_acc=accuracy_score(labels.detach().cpu(), preds)\n",
    "        self.log(\"val_acc\", val_acc, prog_bar=True, logger=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        out = self.backbone(batch)\n",
    "        return out\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        param_optimizer = list(self.backbone.named_parameters()) # self.model.named_parameters()\n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_parameters = [\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\": 1e-6,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        optimizer = torch.optim.AdamW(optimizer_parameters, lr=self.hparams.learning_rate)\n",
    "        scheduler_cosie = CosineAnnealingLR(optimizer, T_max= 10, eta_min=1e-6, last_epoch=-1)\n",
    "        # scheduler_warmup = GradualWarmupSchedulerV2(optimizer, multiplier=1, total_epoch=5, after_scheduler=scheduler_cosie)\n",
    "        return dict(optimizer=optimizer, lr_scheduler=scheduler_cosie) # , lr_scheduler=scheduler_warmup lr_scheduler=scheduler[optimizer], [scheduler]\n",
    "\n",
    "class MyDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        TRAIN_DF,\n",
    "        VALID_DF,\n",
    "        batch_size: int = 32,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        trn_dataset = WavDatset(TRAIN_DF, trans=get_transforms(data='train')) \n",
    "        val_dataset = WavDatset(VALID_DF, trans=get_transforms(data='valid')) \n",
    "        \n",
    "        self.train_dset = trn_dataset\n",
    "        self.valid_dset = val_dataset\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dset, batch_size=self.batch_size, shuffle=True, num_workers=8)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.valid_dset, batch_size=self.batch_size, shuffle=False, num_workers=8) \n",
    "\n",
    "def cli_main():\n",
    "    logger = WandbLogger(name=f'CNN_mobilev3_224448_fold_{C_FOLD}', project='sp_recog')\n",
    "    classifier =  LitClassifier()\n",
    "    mc = ModelCheckpoint('model_mobilev3_224448', monitor='val_acc', mode='max', filename='{epoch}-{val_acc:.4f}_' + f'fold_{C_FOLD}')\n",
    "    # swa = StochasticWeightAveraging(swa_epoch_start=2, annealing_epochs=2)\n",
    "    trainer = pl.Trainer(\n",
    "            gpus=1,\n",
    "            max_epochs=100,\n",
    "            # accelerator='ddp_spawn',\n",
    "            # stochastic_weight_avg=True,\n",
    "            callbacks=[mc],\n",
    "            logger=logger\n",
    "            )\n",
    "    mydatamodule = MyDataModule(TRAIN_DF, VALID_DF)\n",
    "    trainer.fit(classifier, datamodule=mydatamodule)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Traninig"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "if DEBUG:\n",
    "    print(\"======== DEBUG MODE =========\")\n",
    "    train_df = pd.read_csv('../data/train_label.csv')[:1000]\n",
    "else:\n",
    "    train_df = pd.read_csv('../data/train_label.csv')\n",
    "\n",
    "# split fold\n",
    "C_FOLD = 0\n",
    "skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "for i, (train_idx, valid_idx) in enumerate(skf.split(X=train_df, y=train_df['age_'])):\n",
    "\n",
    "    if i == C_FOLD:\n",
    "        TRAIN_DF = train_df.iloc[train_idx]\n",
    "\n",
    "        VALID_DF = train_df.iloc[valid_idx]\n",
    "\n",
    "seed_everything()\n",
    "cli_main()\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Load Train mode dataset ...\n",
      "ThreadPool Elapsed 428.2954671382904\n",
      "Load Train mode dataset ...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ThreadPool Elapsed 106.62432932853699\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mygs\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">CNN_mobilev3_224448_fold_0</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/ygs/sp_recog\" target=\"_blank\">https://wandb.ai/ygs/sp_recog</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/ygs/sp_recog/runs/3kit7w2k\" target=\"_blank\">https://wandb.ai/ygs/sp_recog/runs/3kit7w2k</a><br/>\n",
       "                Run data is saved locally in <code>/home/yilgukseo/DL/Voice/AI_hub_pre/src/wandb/run-20210929_213413-3kit7w2k</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | backbone  | ImgModel         | 5.3 M \n",
      "1 | criterion | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "5.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.3 M     Total params\n",
      "21.188    Total estimated model params size (MB)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 14:  76%|███████▌  | 602/791 [01:34<00:29,  6.37it/s, loss=0.0516, v_num=7w2k, acc=0.938, Train Loss=0.150, val_loss_step=0.109, val_loss_epoch=1.400, val_acc=0.584] "
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/yilgukseo/anaconda3/envs/AI_hub_pre/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1046: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 14:  76%|███████▌  | 602/791 [01:50<00:34,  5.45it/s, loss=0.0516, v_num=7w2k, acc=0.938, Train Loss=0.150, val_loss_step=0.109, val_loss_epoch=1.400, val_acc=0.584]"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def make_test_dataloader():\n",
    "    # Test Dataloader\n",
    "    test_df = pd.read_csv('../data/sample_submission.csv')#[:100]\n",
    "    test_dataset = WavDatset(test_df, trans=get_transforms(data='test'), is_train=False) #  images=test_images,\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=10)\n",
    "    return test_dataloader"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "\n",
    "class LitClassifier(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    >>> LitClassifier(Backbone())  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\n",
    "    LitClassifier(\n",
    "      (backbone): ...\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        scale_list = [0.25, 0.5], # 0.125, \n",
    "        backbone: Optional[ImgModel] = None,\n",
    "        learning_rate: float = 0.001,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=['backbone'])\n",
    "        if backbone is None:\n",
    "            backbone = ImgModel()\n",
    "        self.backbone = backbone\n",
    "\n",
    "    def forward(self, batch):\n",
    "        out = self.backbone.backbone.forward_features(batch)\n",
    "        out = self.backbone.pool(out)\n",
    "        out = self.backbone.fc(out[:,:,0,0])\n",
    "        return out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "\n",
    "weight_file = 'epoch=7-val_acc=0.5946_fold_0.ckpt' \n",
    "loade_model_path = f'../src/model_mobilev3_224448/{weight_file}'\n",
    "model = LitClassifier()\n",
    "model = model.load_from_checkpoint(loade_model_path).cuda().eval()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "test_dataloader = make_test_dataloader()\n",
    "pred_list = list()\n",
    "with torch.no_grad():\n",
    "    for i, x in enumerate(tqdm(test_dataloader)):\n",
    "        pred = model(x.cuda())\n",
    "        pred = pred.detach().cpu()#.sigmoid()\n",
    "        pred = torch.argmax(pred, dim=1)\n",
    "        pred_list.append(pred)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Load Inference mode dataset ...\n",
      "Epoch 14:  76%|███████▌  | 602/791 [02:31<00:47,  3.97it/s, loss=0.0516, v_num=7w2k, acc=0.938, Train Loss=0.150, val_loss_step=0.109, val_loss_epoch=1.400, val_acc=0.584]\n",
      "ThreadPool Elapsed 36.91880011558533\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 16/16 [00:03<00:00,  4.02it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "pred = torch.cat(pred_list)\n",
    "len(pred)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1990"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "sns.countplot(pred.numpy());"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/yilgukseo/anaconda3/envs/AI_hub_pre/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "sub = pd.read_csv('../data/sample_submission.csv')\n",
    "sub['age_'] = pred.numpy() + 1\n",
    "sub.to_csv('../sub/mel_b0_224_448size.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "sub['age_'].value_counts(normalize=True)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3    0.425126\n",
       "2    0.216583\n",
       "4    0.157286\n",
       "5    0.118593\n",
       "6    0.082412\n",
       "Name: age_, dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.0",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit ('AI_hub_pre': conda)"
  },
  "interpreter": {
   "hash": "66517f3afaadadeff9f8009b99a590195ad9dd1e6b56106f4b6e20b4b1416649"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}